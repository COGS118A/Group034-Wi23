{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Models and Methods: Predictions of Client Bank Term Deposit Subscription Likelihood\n",
    "\n",
    "# Names\n",
    "\n",
    "- Brooks (Ruijia) Niu\n",
    "- Hannah Lin\n",
    "- Xiaofei Teng\n",
    "- Jiaying Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Our goal is to utilize our dataset Bank Marketing, from the UCI Machine Learning Repository<a name=\"dataset\"></a>[<sup>[1]</sup>](#ucidatanote), to predict whether a given client will subscribe to a term deposit. A term deposit subscription is essentially the depositing of a fixed amount of money into an account for a predetermined length of time. This is typically offered by financial institutions such as banks and credit unions. The dataset consists of 45,211 observations and 17 variables containing client bank data, bank campaigning, and socio-economic context. We will use the data to create a classification model to predict whether an individual, given the characteristics, is likely to subscribe to a term deposit. The model will be evaluated by accuracy and a confusion matrix/ROC Curve. An accurate model can be deemed useful by bank telemarketing campaigners to determine which clients should be targeted in their advertisements for term deposit subscriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Machine learning techniques have gained more attention recently in a number of businesses, including the banking industry, as a way to forecast client behavior. Predicting whether clients would sign up for a term deposit is one such application that can help banks better target and develop their customer acquisition and retention strategy.\n",
    "\n",
    "Machine learning algorithms for forecasting term deposit subscriptions have been the subject of numerous studies. In a study by Moro et al. (2014)<a name=\"moro2014\"></a>[<sup>[2]</sup>](#moro2014note), for instance, classification techniques like logistic regression, decision trees, and random forests were used to predict term deposit subscriptions using data from a Portuguese bank's marketing efforts. According to the analysis, random forests had the highest accuracy, coming in at 81%. Though the performance is already impressive, the bank is pursuing even higher accuracy for the profit it could gain.\n",
    "\n",
    "In order to increase the precision of term deposit subscription prediction models, other studies have concentrated on feature selection and engineering. For instance, Moro et al. (2012)<a name=\"moro2012\"></a>[<sup>[3]</sup>](#moro2012) demonstrated the potential of how data mining techniques can enhance bank direct marketing. Their study focused on identifying the most effective customer attributes and behaviors for targeted marketing campaigns, and their results showed that data mining could significantly improve campaign effectiveness.\n",
    "\n",
    "In another study, Nobibon et al. (2011)<a name=\"nobibon2011\"></a>[<sup>[4]</sup>](#nobibon2011note) developed optimization models for targeted offers in direct marketing using both exact and heuristic algorithms. Their models aimed to determine the optimal offers to present to customers based on their purchase histories and demographic characteristics, and their results showed that their models outperformed existing marketing strategies.\n",
    "\n",
    "Therefore for the banking industry, using machine learning approaches to forecast term deposit subscriptions has produced encouraging results in terms of enhancing the efficiency of marketing efforts and client acquisition and retention tactics. Encouraged by past studies, our team aims to apply machine learning techniques learned from class to classify real bank marketing datasets and analyze the deterministic attributes of clients that will sign up for deposits.\n",
    "\n",
    "Similar to Moro et. al. (2012) who used: logistic regression, decision trees (DTs), neural network (NN) and support vector machines, we will be utilizing those former methods and additionally K-NN and random forest, as well as other possibilities.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project, the problem we are trying to solve is that given some personal information of a new client, we try to predict whether he or she is likely to subscribe a term deposit. Given the property of our task, we would like to maximize the recall rate, on the basis that we make good enough prediction overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this project, we will be using the Bank Marketing Data Set from UCI MAchine Learning Repository. The data is public and can be accessed through the following link: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing.\n",
    "This data set recorded information of individual clients and whether each of them has subscribed a term deposit. The following information is from the data set website.\n",
    "\n",
    "Data Set Information:\n",
    "\tThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "\n",
    "We are using bank-full.csv as our dataset, which has 45,211 instances, each one has the following 17 attributes:\n",
    "- 1 - age (numeric)\n",
    "- 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "- 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "- 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "- 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "- 6 - balance: bank balance (numeric)\n",
    "- 7 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "- 8 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "- 9 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "- 10 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "- 11 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "- 12 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "- 13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "- 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n",
    "- 15 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "- 16 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','unknown','success', ‘other’)\n",
    "- 17 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "Some of the attributes, like age or balance, could possibly be more significant than others, like last contact day or month of the year. \n",
    "There are several categorical attributes, which we will apply further transformations such as one-hot encoding before we proceed to the next step. For categorical data that includes an “unknown” feature, we plan to introduce soft one-hot encoding that treats “unknown” as a event that has equal probability over the probability space (for example, if there are 3 possible outcomes except for “unknown”, then “unknown” would be coded as 0.33, 0.33, 0.33. \n",
    "For numerical data, because the scales across features vary a lot, we plan to normalize the data according to the largest value in specific features, so that all the numerical values are between 0 and 1. For feature 14, pdays, there is an exception for clients that are not previously contacted, and the assigned value for such exception is -1, which has no particular meaning. We plan to code it the same way as the largest possible value under that feature and then normalize it (so that all -1 become 1 after normalization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this project, we aim to develop a prediction model for determining whether the client will subscribe to a term deposit in marketing campaigns. Because of the multivariate nature of our dataset (20 inputs with numerical and categorical value) and the binary nature of our prediction task (whether the user will subscribe or not), we will implement K-NN, decision tree, random forest, logistic regression and support vector machine (SVM) and explore other possibilities. We will also use gridsearch to find good hyper parameters. These solutions will be tested with a test dataset separated from the entire dataset and not involved in training. The train-test split ratio is 8:2 since we have a large enough dataset (the test data has 9042 entries) to ensure the robustness of the test dataset.\n",
    "\n",
    "We will not be using cross validation because we have a large dataset and we are implementing grid search, the training time will skyrocket if we are combining both. In spite of the training time, the test data already has enough entries to ensure robustness. Performance will be measured  with evaluation metrics in the next section. For coding, we will mainly take advantage of scikit-learn; for example, sklearn.neighbors.KNeighborsClassifier for K-NN,  sklearn.tree.DecisionTreeClassifier for decision tree, sklearn.ensemble.RandomForestClassifier for random forest, sklearn.linear_model.LogisticRegression for logistic regression, sklearn.svm.SVC for support vector machine and sklearn.model_selection.GridSearchCV for grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For evaluation, we will utilize confusion matrix to intuitively analyze the discrepancy between our models’ predictions and the ground truth. The confusion matrix will be of shape 2x2 and cells containing true positives, false positives, false negatives and true negatives. We will also take advantage of the AUC-ROC curve to make sure our models have good classification threshold. We can then fine tune our models to maximize specificity / precision based on our demand. Another evaluation metric will be prediction accuracy, which is the ratio of correct predictions out of all predictions. All the models will be tested on the same test dataset that is randomly splitted from the shuffled full dataset before training. The size of test data will be 25% of all data and the rest of the data will be used for training. We picked this ratio considering we have sufficient data. Thus, a large test dataset will improve our model’s robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import zero_one_loss, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"bank/bank-full.csv\", delimiter= ';')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered this data in the University of California, Irvine Machine Learning Repository. The dataset was collected from “A Data-Driven Approach to Predict the Success of Bank Telemarketing” (Moro et al, 2014).\n",
    "\n",
    "In any machine learning project, it is crucial to take into account potential ethical dilemmas and data privacy concerns. For instance, the usage of personal data prompts concerns about individuals' privacy and the security of their data. Moreover, using some algorithms can lead to biased judgments, which might result in unjust treatment or discrimination against particular groups if the algorithm has determined they are more / less likely to sign up for a deposit. This particular dataset collected by Moro et al. covers more than 45,000 instances and to ensure their privacy, the participants’ names and locations were not collected at the time. At the time of collection, informed consent was given. \n",
    "\n",
    "Our team will take measures to address these ethical concerns further to make sure that data is handled ethically. We will also regularly train our model on the best practices to avoid biased machine learning. We will make use of tools like the Ethics Checklist to help us in our endeavors. We seek to build an ethical and efficient machine-learning project by proactively tackling potential ethical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show thorough understanding of all aspects of the project and concepts learned in the course; If not, reach out to fellow group members to do so\n",
    "- All members should be contributing equally across the entire project submission\n",
    "- Writing well-commented and clear code to wrangle, explore, visualize, analyze, and communicate our findings; Writing the accompanying text throughout the project to explain each section\n",
    "- Editing the text and code throughout our project for grammar, misspellings, and clarity\n",
    "- Be punctual to group scheduled meetings\n",
    "- Effectively communicate to all group members if there is a problem or emergency as soon as possible\n",
    "- Each member should complete their assigned task by the given deadline\n",
    "- Be respectful and open-minded while other members are sharing their opinions and feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/21  |  11 AM |  Get in contact with all group members on WeChat  | Determine the best form of communication; Discuss and decide on the final project topic; Assign sections of Project Proposal to be completed | \n",
    "| 2/22  |  Before 11:59 PM |  Complete Project Proposal draft, Visit Office hours for feedback on Project Proposa | Review and make final edits on Project Proposal. Update information to the Jupiter notebook. Submit the Project Proposal. | \n",
    "| 2/28  | 11 AM  | N/A  | Import and wrangle data; EDA; Work on model. Assign group members to lead each specific part.  |\n",
    "| 3/2  | 11 AM  | Finalize wrangling/EDA; Begin programming for project  | Review/Edit wrangling/EDA; Discuss Analysis Plan; Work on the model.  |\n",
    "| 3/7  | 11 AM  | Edit/Update the Project Checkpoint | Review, discuss, and edit project code. Discuss; Complete Project Checkpoint |\n",
    "| 3/8  | Before 11:59 PM  | Complete Project Checkpoint draft, Visit Office hours for feedback on Project | Discuss/edit project code; Complete project |\n",
    "| 3/14  | 11 AM  | Complete analysis; Draft results/conclusion/discussion | Discuss and make edits on Final Project; If there’s extra time implement a neural network model of project and compare with current model.  |\n",
    "| 3/22  | Before 11:59 PM  | N/A | Turn in Final Project/ Evaluation Survey  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"ucidatanote\"></a>1.[^](#dataset): Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "<a name=\"moro2014note\"></a>2.[^](#moro2014): [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "<a name=\"moro2012note\"></a>3.[^](#moro2012):Sérgio Moro, Raul Laureano, Paulo Cortez\n",
    "Enhancing bank direct marketing through data mining\n",
    "Proceedings of the Forty-First International Conference of the European Marketing Academy, European Marketing Academy (2012), pp. 1-8\n",
    "\n",
    "<a name=\"nobibon2011note\"></a>4.[^](#nobibon2011):Fabrice Talla Nobibon, Roel Leus, Frits CR Spieksma\n",
    "Optimization models for targeted offers in direct marketing: exact and heuristic algorithms\n",
    "European Journal of Operational Research, 210 (3) (2011), pp. 670-683\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
